{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandar-solanki/GAN-exercises/blob/main/GAN_1021_Day_1_GAN_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "XzHoHLxHt2vS"
      },
      "id": "XzHoHLxHt2vS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Github Repo\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "%cd gdrive/My Drive/project_folder/AI-Fundamentals-Updated\n",
        "! git pull\n"
      ],
      "metadata": {
        "id": "5p3gWAYpt3d_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e35494e-78b1-4bb3-9102-6ca7aed5396e",
        "collapsed": true
      },
      "id": "5p3gWAYpt3d_",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/project_folder/AI-Fundamentals-Updated\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 851.42 KiB | 5.01 MiB/s, done.\n",
            "From https://github.com/mandar-solanki/AI-Fundamentals-Updated\n",
            "   61be1b2..5e782cf  main       -> origin/main\n",
            "Updating 61be1b2..5e782cf\n",
            "Fast-forward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de606e3c",
      "metadata": {
        "id": "de606e3c"
      },
      "source": [
        "# Handwritten Digits Generator With a GAN: https://realpython.com/generative-adversarial-networks/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c1567d",
      "metadata": {
        "id": "98c1567d"
      },
      "source": [
        "<span style=\"font-family:Papyrus; font-size:2.5em;\">Generative Adversarial Networks (GANs)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8434ee4",
      "metadata": {
        "id": "e8434ee4"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "- Generative Adversarial Networks (GANs)\n",
        "- How GANs Work\n",
        "- GANs Process\n",
        "- Examples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Generative Adversarial Networks (GANs)\n",
        "\n",
        "Generative Adversarial Networks are used to generate images that never existed before. They learn about the world (objects, animals and so forth) and create new versions of those images that never existed.\n",
        "\n",
        "They have two components:\n",
        "\n",
        "- A **Generator** - this creates the images.\n",
        "- A **Discriminator** - this assesses the images and tells the generator if they are similar to what it has been trained on. These are based off real world examples.\n",
        "\n",
        "When training the network, both the generator and discriminator start from scratch and learn together.\n",
        "\n",
        "### How GANs Work\n",
        "\n",
        "\n",
        "**G** for **Generative** - this is a model that takes an input as a random noise singal and then outputs an image.\n",
        "\n",
        "\n",
        "\n",
        "**A** for **Adversarial** - this is the discriminator, the opponent of the generator. This is capable of learning about objects, animals or other features specified. For example: if you supply it with pictures of dogs and non-dogs, it would be able to identify the difference between the two.\n",
        "\n",
        "\n",
        "Using this example, once the discriminator has been trained, showing the discriminator a picture that isn't a dog it will return a 0. Whereas, if you show it a dog it will return a 1.\n",
        "\n",
        "\n",
        "**N** for **Network** - meaning the generator and discriminator are both neural networks.\n",
        "\n",
        "\n",
        "### GANs Process\n",
        "\n",
        "**Step 1** - we input a random noise signal into the generator. The generator creates some images which is used for training the discriminator. We provide the discriminator with some features/images we want it to learn and the discriminator outputs probabilities. These probabilities can be rather high as the discriminator has only just started being trained. The values are then assessed and identified. The error is calculated and these are backpropagated through the discriminator, where the weights are updated.\n",
        "\n",
        "\n",
        "Next we train the generator. We take the batch of images that it created and put them through the discriminator again. We do not include the feature images. The generator learns by tricking the discriminator into it outputting false positives.\n",
        "\n",
        "The discriminator will provide an output of probabilities. The values are then assessed and compared to what they should have been. The error is calculated and backpropagated through the generator and the weights are updated.\n",
        "\n",
        "\n",
        "**Step 2** - This is the same as step 1 but the generator and discriminator are trained a little more. Through backpropagation the generator understands its mistakes and starts to make them more like the feature.\n",
        "\n",
        "This is created through a *Deconvolutional Neural Network*.\n",
        "\n",
        "### Examples\n",
        "\n",
        "**GANs** can be used for the following:\n",
        "\n",
        "- Generating Images\n",
        "- Image Modification\n",
        "- Super Resolution\n",
        "- Assisting Artists\n",
        "- Photo-Realistic Images\n",
        "- Speech Generation\n",
        "- Face Ageing\n",
        "\n",
        "<br>\n",
        "**[It’s Training Cats and Dogs: NVIDIA Research Uses AI to Turn Cats Into Dogs, Lions and Tigers, Too](https://blogs.nvidia.com/blog/2018/04/15/nvidia-research-image-translation/)**\n",
        "\n",
        "<img src=\"https://blogs.nvidia.com/wp-content/uploads/2018/04/cats-dogs-nvresearch1.png\" height=\"700\" width=\"500\">\n",
        "<br>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*HaExieykcOT5oI2_xKisrQ.png\" height=\"700\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b8e8db",
      "metadata": {
        "id": "41b8e8db"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ea95cd",
      "metadata": {
        "id": "04ea95cd",
        "outputId": "25dee4a4-d795-4e75-a765-45cd86289f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7dd933ae5b10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.manual_seed(111)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a31a789e",
      "metadata": {
        "id": "a31a789e"
      },
      "source": [
        "# Preparing the Training Data\n",
        "The MNIST dataset consists of 28 × 28 pixel grayscale images of handwritten digits from 0 to 9. To use them with PyTorch, you’ll need to perform some conversions. For that, you define transform, a function to be used when loading the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb49d31",
      "metadata": {
        "id": "4fb49d31"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb0eac5",
      "metadata": {
        "id": "0fb0eac5"
      },
      "outputs": [],
      "source": [
        "train_set = torchvision.datasets.MNIST(\n",
        "    root=\".\", train=True, download=True, transform=transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4960bf01",
      "metadata": {
        "id": "4960bf01"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774506f2",
      "metadata": {
        "scrolled": false,
        "id": "774506f2"
      },
      "outputs": [],
      "source": [
        "real_samples, mnist_labels = next(iter(train_loader))\n",
        "for i in range(16):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(real_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289ba5e5",
      "metadata": {
        "id": "289ba5e5"
      },
      "source": [
        "# Implementing the Discriminator and the Generator\n",
        "In this case, the discriminator is an MLP neural network that receives a 28 × 28 pixel image and provides the probability of the image belonging to the real training data.\n",
        "\n",
        "You can define the model with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432bf6e3",
      "metadata": {
        "id": "432bf6e3"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), 784)\n",
        "        output = self.model(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c0f84d9",
      "metadata": {
        "id": "3c0f84d9"
      },
      "source": [
        "To input the image coefficients into the MLP neural network, you vectorize them so that the neural network receives vectors with 784 coefficients.\n",
        "\n",
        "The vectorization occurs in the first line of .forward(), as the call to x.view() converts the shape of the input tensor. In this case, the original shape of the input x is 32 × 1 × 28 × 28, where 32 is the batch size you’ve set up. After the conversion, the shape of x becomes 32 × 784, with each line representing the coefficients of an image of the training set.\n",
        "\n",
        "To run the discriminator model using the GPU, you have to instantiate it and send it to the GPU with .to(). To use a GPU when there’s one available, you can send the model to the device object you created earlier:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfde368b",
      "metadata": {
        "id": "bfde368b"
      },
      "outputs": [],
      "source": [
        "device = \"\"\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe8950f",
      "metadata": {
        "id": "afe8950f"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator().to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce0b9015",
      "metadata": {
        "id": "ce0b9015"
      },
      "source": [
        "Since the generator is going to generate more complex data, it’s necessary to increase the dimensions of the input from the latent space. In this case, the generator is going to be fed a 100-dimensional input and will provide an output with 784 coefficients, which will be organized in a 28 × 28 tensor representing an image.\n",
        "\n",
        "Here’s the complete generator model code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff3d8fe",
      "metadata": {
        "id": "4ff3d8fe"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        output = output.view(x.size(0), 1, 28, 28)\n",
        "        return output\n",
        "\n",
        "generator = Generator().to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb94b10",
      "metadata": {
        "id": "5fb94b10"
      },
      "source": [
        "n line 12, you use the hyperbolic tangent function Tanh() as the activation of the output layer since the output coefficients should be in the interval from -1 to 1. In line 20, you instantiate the generator and send it to device to use the GPU if one is available.\n",
        "\n",
        "Now that you have the models defined, you’ll train them using the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2159c0f9",
      "metadata": {
        "id": "2159c0f9"
      },
      "source": [
        "# Training the Models\n",
        "To train the models, you need to define the training parameters and optimizers like you did in the previous example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0efaab",
      "metadata": {
        "id": "6e0efaab"
      },
      "outputs": [],
      "source": [
        "lr = 0.0001\n",
        "num_epochs = 50\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e0318f",
      "metadata": {
        "id": "d2e0318f"
      },
      "source": [
        "To obtain a better result, you decrease the learning rate from the previous example. You also set the number of epochs to 50 to reduce the training time.\n",
        "\n",
        "The training loop is very similar to the one you used in the previous example. In the highlighted lines, you send the training data to device to use the GPU if available:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e4a07c",
      "metadata": {
        "id": "a7e4a07c"
      },
      "source": [
        "# MIN max Apporach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "906138a4",
      "metadata": {
        "id": "906138a4"
      },
      "source": [
        "The min-max approach in Generative Adversarial Networks (GANs) refers to the original objective function proposed by Ian Goodfellow and his colleagues in the seminal GAN paper. This objective function involves two components: a minimization (min) and a maximization (max) that are played out between two neural networks, the generator and the discriminator.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. **Generator (G)**: This network takes random noise as input and tries to generate data samples that are indistinguishable from real data.\n",
        "\n",
        "2. **Discriminator (D)**: This network tries to distinguish between real data samples and fake data samples generated by the generator.\n",
        "\n",
        "The objective of GANs is to train both networks simultaneously such that the generator gets better at generating realistic data, while the discriminator gets better at distinguishing real from fake data. This is achieved through a min-max game:\n",
        "\n",
        "- **Minimization (Generator)**: The generator aims to minimize the probability that the discriminator correctly classifies its generated samples as fake. In other words, it wants to fool the discriminator into thinking its generated samples are real. The generator's loss function is the negative of the discriminator's log probability of making a mistake:\n",
        "\n",
        "\n",
        "- **Maximization (Discriminator)**: The discriminator aims to maximize its ability to correctly classify real and fake samples. Its loss function is the sum of two terms: the log probability of correctly classifying real samples and the log probability of correctly classifying fake samples:\n",
        "\n",
        "\n",
        "\n",
        "The min-max game continues iteratively, with the generator and discriminator updating their weights based on their respective loss functions. As training progresses, the generator learns to generate more realistic data, while the discriminator becomes better at distinguishing between real and fake samples.\n",
        "\n",
        "The min-max approach of GANs has been highly influential and has led to many advancements in generative modeling and deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a42a535",
      "metadata": {
        "id": "1a42a535"
      },
      "source": [
        "`zero_grad()` is a method typically used in deep learning frameworks like PyTorch or TensorFlow to reset the gradients of the parameters of a model to zero. This operation is necessary before computing the gradients for a new batch of data during the training process. By zeroing out the gradients, you ensure that the gradients from the previous batch do not accumulate and affect the current batch's gradient computation. This helps in maintaining the correct gradient values for each batch and prevents gradient values from becoming too large or too small, which can lead to training instability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c2e605",
      "metadata": {
        "id": "52c2e605"
      },
      "source": [
        "1. Move Real Samples to Device: The real_samples tensor contains a batch of real data samples. This line moves these samples to the specified device (e.g., CPU or GPU) for computation.\n",
        "\n",
        "2. Create Labels for Real Samples: The real_samples_labels tensor contains labels for the real samples. In GANs, these labels are typically set to 1 to indicate that these samples are real.\n",
        "\n",
        "3. Generate Random Noise: The latent_space_samples tensor contains randomly generated noise. This noise is used as input to the generator network to produce fake data samples.\n",
        "\n",
        "4. Generate Fake Samples: The generated_samples tensor contains fake data samples generated by the generator network using the random noise from latent_space_samples.\n",
        "\n",
        "5. Create Labels for Fake Samples: The generated_samples_labels tensor contains labels for the generated (fake) samples. In GANs, these labels are typically set to 0 to indicate that these samples are fake.\n",
        "\n",
        "6. Concatenate Real and Fake Samples: The all_samples tensor is created by concatenating the real and generated samples along the batch dimension. This creates a batch of samples containing both real and fake data.\n",
        "\n",
        "7. Concatenate Real and Fake Labels: The all_samples_labels tensor is created by concatenating the real and generated sample labels along the batch dimension. This creates a corresponding batch of labels indicating whether each sample is real (1) or fake (0).\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2806725e",
      "metadata": {
        "id": "2806725e"
      },
      "source": [
        "# Train the discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d4edfe9",
      "metadata": {
        "id": "5d4edfe9"
      },
      "source": [
        "1. discriminator.zero_grad(): This line clears the gradients of the discriminator's parameters. Gradients are used to update the weights of the neural network during training, and it's important to clear them before computing new gradients in each iteration of the training loop.\n",
        "\n",
        "\n",
        "2. output_discriminator = discriminator(all_samples): This line passes all the data samples (both real and generated) through the discriminator to obtain its predictions. The discriminator outputs a probability score for each sample, indicating how likely it thinks the sample is real.\n",
        "\n",
        "\n",
        "3. loss_discriminator = loss_function(output_discriminator, all_samples_labels): Here, the code calculates the loss (or error) of the discriminator's predictions. The loss_function is typically a binary cross-entropy loss, which measures the difference between the predicted probabilities and the actual labels (0 for fake samples, 1 for real samples).\n",
        "\n",
        "\n",
        "4. loss_discriminator.backward(): This line computes the gradients of the loss with respect to the discriminator's parameters, using backpropagation. These gradients will be used to update the discriminator's weights in the next step.\n",
        "\n",
        "\n",
        "5. optimizer_discriminator.step(): Finally, the optimizer (e.g., Adam or SGD) updates the discriminator's weights based on the computed gradients, using the learning rate and other optimizer settings. This step is what actually improves the discriminator's ability to distinguish between real and fake samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d18d41e8",
      "metadata": {
        "id": "d18d41e8"
      },
      "source": [
        "# Train the generator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5800e6e4",
      "metadata": {
        "id": "5800e6e4"
      },
      "source": [
        "1. generator.zero_grad(): This line clears the gradients of the generator's parameters. This step is necessary because gradients need to be recalculated for each batch of data samples.\n",
        "\n",
        "\n",
        "2. generated_samples = generator(latent_space_samples): The generator takes random noise (latent space samples) as input and generates synthetic data samples. These samples will be used to train the generator to produce more realistic data.\n",
        "\n",
        "\n",
        "3. output_discriminator_generated = discriminator(generated_samples): The generated samples are passed through the discriminator to obtain its predictions. The discriminator's output represents how likely it thinks the generated samples are real.\n",
        "\n",
        "\n",
        "4. loss_generator = loss_function(output_discriminator_generated, real_samples_labels): This line calculates the loss (or error) of the generator based on the discriminator's predictions for the generated samples. The goal of the generator is to generate samples that the discriminator classifies as real, so the loss is calculated based on the discriminator's output compared to the label indicating the samples are real.\n",
        "\n",
        "\n",
        "5. loss_generator.backward(): Backpropagation is used to compute the gradients of the loss with respect to the generator's parameters. These gradients will be used to update the generator's weights in the next step.\n",
        "\n",
        "\n",
        "6. optimizer_generator.step(): The optimizer (e.g., Adam or SGD) updates the generator's weights based on the computed gradients, using the learning rate and other optimizer settings. This step aims to improve the generator's ability to generate realistic data samples over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1483716a",
      "metadata": {
        "id": "1483716a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for n, (real_samples, mnist_labels) in enumerate(train_loader):\n",
        "        # Data for training the discriminator\n",
        "        real_samples = real_samples.to(device=device)\n",
        "        real_samples_labels = torch.ones((batch_size, 1)).to(\n",
        "            device=device\n",
        "        )\n",
        "        latent_space_samples = torch.randn((batch_size, 100)).to(\n",
        "            device=device\n",
        "        )\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        generated_samples_labels = torch.zeros((batch_size, 1)).to(\n",
        "            device=device\n",
        "        )\n",
        "        all_samples = torch.cat((real_samples, generated_samples))\n",
        "        all_samples_labels = torch.cat(\n",
        "            (real_samples_labels, generated_samples_labels)\n",
        "        )\n",
        "\n",
        "        # Training the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        output_discriminator = discriminator(all_samples)\n",
        "        loss_discriminator = loss_function(\n",
        "            output_discriminator, all_samples_labels\n",
        "        )\n",
        "        loss_discriminator.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        # Data for training the generator\n",
        "        latent_space_samples = torch.randn((batch_size, 100)).to(\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Training the generator\n",
        "        generator.zero_grad()\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        output_discriminator_generated = discriminator(generated_samples)\n",
        "        loss_generator = loss_function(\n",
        "            output_discriminator_generated, real_samples_labels\n",
        "        )\n",
        "        loss_generator.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        # Show loss\n",
        "        if n == batch_size - 1:\n",
        "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
        "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n",
        "\n",
        "    latent_space_samples = torch.randn(batch_size, 100).to(device=device)\n",
        "    generated_samples = generator(latent_space_samples)\n",
        "\n",
        "    generated_samples = generated_samples.cpu().detach()\n",
        "    for i in range(16):\n",
        "        ax = plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(generated_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b994b2f5",
      "metadata": {
        "id": "b994b2f5"
      },
      "source": [
        "Some of the tensors don’t need to be sent to the GPU explicitly with device. This is the case with generated_samples in line 11, which will already be sent to an available GPU since latent_space_samples and generator were sent to the GPU previously.\n",
        "\n",
        "Since this example features more complex models, the training may take a bit more time. After it finishes, you can check the results by generating some samples of handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ac88b43",
      "metadata": {
        "id": "6ac88b43"
      },
      "source": [
        "# Checking the Samples Generated by the GAN\n",
        "To generate handwritten digits, you have to take some random samples from the latent space and feed them to the generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e447384",
      "metadata": {
        "id": "1e447384"
      },
      "outputs": [],
      "source": [
        "latent_space_samples = torch.randn(batch_size, 100).to(device=device)\n",
        "generated_samples = generator(latent_space_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea8fa2d",
      "metadata": {
        "id": "7ea8fa2d"
      },
      "source": [
        "To plot generated_samples, you need to move the data back to the CPU in case it’s running on the GPU. For that, you can simply call .cpu(). As you did previously, you also need to call .detach() before using Matplotlib to plot the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db3c9434",
      "metadata": {
        "id": "db3c9434"
      },
      "source": [
        "# Generation of Synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe3be63",
      "metadata": {
        "id": "efe3be63",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "generated_samples = generated_samples.cpu().detach()\n",
        "for i in range(16):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(generated_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3810c9",
      "metadata": {
        "id": "2a3810c9"
      },
      "source": [
        "After fifty epochs of training, there are several generated digits that resemble the real ones. You can improve the results by considering more training epochs. As with the previous example, by using a fixed latent space samples tensor and feeding it to the generator at the end of each epoch during the training process, you can visualize the evolution of the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceacc1f3",
      "metadata": {
        "id": "ceacc1f3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}